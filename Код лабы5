import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

def load_data(filename):
    data = np.loadtxt(filename, delimiter=',')
    X = data[:, :2]  # Первые два столбца
    y = data[:, 2]   # Третий столбец
    return X, y

def plot_data(X, y):
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Not Accepted')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Accepted')
    plt.xlabel('Test 1 Score')
    plt.ylabel('Test 2 Score')
    plt.legend()
    plt.show()

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def map_feature(X1, X2):
    degree = 6
    m = X1.size
    out = np.ones((m, 1))  # Столбец единиц
    for i in range(1, degree + 1):
        for j in range(0, i + 1):
            out = np.c_[out, (X1 ** (i - j)) * (X2 ** j)]
    return out

def cost_function(theta, X, y, lamb):
    m = len(y)
    h = sigmoid(X @ theta)
    J = (-1/m) * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h)) + (lamb / (2 * m)) * np.sum(np.square(theta[1:]))
    return J

def gradient_function(theta, X, y, lamb):
    m = len(y)
    h = sigmoid(X @ theta)
    grad = (1/m) * (X.T @ (h - y))
    grad[1:] += (lamb / m) * theta[1:]  # Регуляризация градиента
    return grad

def predict(theta, X):
    probabilities = sigmoid(X @ theta)
    return (probabilities >= 0.5).astype(int)

def main():
    # 1. Загрузка данных
    X, y = load_data('ex2data2.txt')
    plot_data(X, y)

    # Преобразование данных
    X_mapped = map_feature(X[:, 0], X[:, 1])
    
    # 2. Регуляризованная логистическая регрессия
    initial_theta = np.zeros(X_mapped.shape[1])
    lamb = 1  # Параметр регуляризации
    cost = cost_function(initial_theta, X_mapped, y, lamb)
    grad = gradient_function(initial_theta, X_mapped, y, lamb)
    print(f'Initial Cost: {cost}')
    print(f'Initial Gradient: {grad}')

    # 3. Обучение логистической регрессии
    min_res = minimize(fun=cost_function, x0=initial_theta, args=(X_mapped, y, lamb), 
                       jac=gradient_function, method='BFGS')
    theta_optimized = min_res.x
    print(f'Optimized Theta: {theta_optimized[:5]}')  # Печатаем первые 5 элементов

    # Отрисовка границы решения
    plot_decision_boundary(theta_optimized, X_mapped, y)

    # 4. Оценка точности модели
    predictions = predict(theta_optimized, X_mapped)
    accuracy = np.mean(predictions == y) * 100
    print(f'Accuracy: {accuracy}%')

    # Предсказание для новых микрочипов
    new_chip1 = np.array([1, 45, 85, 45**2, 45*85, 85**2, 45**3, 45**2*85, 45*85**2, 85**3, 45**4, 45**3*85, 45**2*85**2, 85**4, 45**5, 45**4*85, 45**3*85**2, 85**5, 45**6, 45**5*85, 45**4*85**2, 85**6, 45**7, 45**6*85, 45**5*85**2, 85**7, 45**8, 45**7*85, 45**6*85**2, 85**8])
    new_chip2 = np.array([1, 30, 70, 30**2, 30*70, 70**2, 30**3, 30**2*70, 30*70**2, 70**3, 30**4, 30**3*70, 30**2*70**2, 70**4, 30**5, 30**4*70, 30**3*70**2, 70**5, 30**6, 30**5*70, 30**4*70**2, 70**6, 30**7, 30**6*70, 30**5*70**2, 70**7, 30**8, 30**7*70, 30**6*70**2, 70**8])
    
    prob1 = sigmoid(new_chip1.dot(theta_optimized))
    prob2 = sigmoid(new_chip2.dot(theta_optimized))
    print(f'Probability for new chip 1: {prob1}')
    print(f'Probability for new chip 2: {prob2}')

    # 5. Влияние регуляризации
    lambdas = [0, 100]  # Параметры регуляризации для переобучения и недообучения
    for l in lambdas:
        min_res = minimize(fun=cost_function, x0=initial_theta, args=(X_mapped, y, l), 
                           jac=gradient_function, method='BFGS')
        theta = min_res.x
        plot_decision_boundary(theta, X_mapped, y, title=f'Regularization Lambda = {l}')

def plot_decision_boundary(theta, X, y, title='Decision Boundary'):
    plt.scatter(X[y == 0][:, 1], X[y == 0][:, 2], color='red', label='Not Accepted')
    plt.scatter(X[y == 1][:, 1], X[y == 1][:, 2], color='blue', label='Accepted')

    # Построение границы решения
    x1_min, x1_max = X[:, 1].min(), X[:, 1].max()
    x2_min, x2_max = X[:, 2].min(), X[:, 2].max()
    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))
    Z = sigmoid(map_feature(xx1.ravel(), xx2.ravel()) @ theta)
    Z = Z.reshape(xx1.shape)

    plt.contour(xx1, xx2, Z, levels=[0.5], linewidths=1, colors='green')
    plt.xlabel('Test 1 Score')
    plt.ylabel('Test 2 Score')
    plt.title(title)
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()
